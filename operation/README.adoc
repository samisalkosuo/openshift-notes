= OpenShift Configuration and Operation
:toc: left
:toc-title: Table of Contents

This document describes how to do various configuration/operational tasks in OpenShift.

== NFS client provisioner

https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner[Kubernetes NFS-Client Provisioner] provides dynamic provisioning using existing NFS server. Great way to provide initial storage class for OpenShift.

=== Online installation

* Follow instructions in https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner.
* After storage class is created, optionally set _managed-nfs-storage_-storageclass as default:
** `oc patch storageclass managed-nfs-storage -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'`

=== Airgapped installation

Since OpenShift does not have access to Internet, extra steps are required to install provisioner.

* Login to jump server.
* Package provisioner image and deplyment files using link:nfs-client-provisioner/package_provisioner_image.sh[save_provisioner_image.sh]:
** `sh package_provisioner_image.sh`
* Copy _nfs-client-provisioner-files.tar_ to bastion server.
* Login to bastion server.
* Extract _nfs-client-provisioner-files.tar_:
** `tar -xf nfs-client-provisioner-files.tar`
* Change to _nfs-client-provisioner_-directory.
* Load provisioner image:
** `podman load < nfs-client-provisioner.tar`
* Tag image:
** `podman tag nfs-client-provisioner external-registry.forum.fi.ibm.com:5001/nfs-client-provisioner:latest`
* Push image:
** `podman push external-registry.forum.fi.ibm.com:5001/nfs-client-provisioner:latest`
* If you have NFS server, you need info about it.
** Or set your own NFS as described in the next section.
* Open _deployment.yaml_.
* Find _image_-entry and change to correct provisioner image. For example:
** `external-registry.forum.fi.ibm.com:5001/nfs-client-provisioner:latest`
* Set IP address and path of your NFS server.
* Login as cluster administrator using oc-client.
* Setup provisioner:
** `oc create -f rbac.yaml`
** `oc create -f deployment.yaml`
** `oc create -f class.yaml`
* Test by creating persistent volume claim:
** `oc create -f test-claim.yaml`
** There should test-claim directory in the NFS server.
* Get storage class:
** `oc get sc`
** And you should see _managed-nfs-storage_-storageclass.
* Optionally set _managed-nfs-storage_-storageclass as default:
** `oc patch storageclass managed-nfs-storage -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'`

==== NFS server

As an example, NFS server can be installed on the bastion server.

* Execute script link:nfs-client-provisioner/setup_nfs_server.sh[setup_nfs_server.sh]:
** `sh setup_nfs_server.sh`
** Script sets up NFS server using directory _/mnt/nfs_share_.
** NFS server allows clients from OpenShift internal network. Internal network is set using _OCP_DHCP_NETWORK_- environment variable set in link:../config.sh[config.sh].

== Image registry

=== Internal registry

Internal image registry is not initially available in OpenShift UPI online or airgapped installations.

* Image registry requires persistent storage before it can be made available.
* Configure storageclass for dynamic provisioning.
** For example, NFS provisioner described previously.
* Configure a default storageclass (like the NFS provisioner).
* Patch image registry operator configuration:
** `oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch '{"spec":{"managementState":"Managed","defaultRoute":true,"storage":{"pvc":{"claim":""}}}}'`
* The patch-command creates also default route for the registry.
** Get default route using command:
** `oc get route default-route -n openshift-image-registry --template='{{ .spec.host }}'`
* Registry can not be used until an identity provider (for example LDAP or HTPasswd) has been configured.
** See later sections about LDAP and HTPasswd identity provider.

=== External registry

External registry is an image registry for containers that should be available for OpenShift but, for any reason, not available from public registry or internal image registry.

The most obvious use case for external registry is for the airgapped OpenShift installation.
The registry is created using script link:external-registry/create-registry.sh[create-registry.sh].

* Create external registry using command:
** `sh create-registry.sh <REGISTRY_NAME> <REGISTRY_DIR> <REGISTRY_PORT> <REGISTRY_CRT_FILE_PATH> <REGISTRY_KEY_FILE_PATH>`
** _REGISTRY_NAME_ is the name of the systemd service.
** _REGISTRY_DIR_ is the full path to registry dir. It is created if it does not exist.
** _REGISTRY_PORT_ is registry port.
** _REGISTRY_CRT_FILE_PATH_ is the full path to certificate file.
** _REGISTRY_KEY_FILE_PATH_ is the full path to certificate key file.
* Registry container is controlled using systemctl.

==== Configure OpenShift

When using external registry in OpenShift, pull secret is required so that pods can pull images from the registry.

Pull secret can be added for a project or it can be added as global cluster pull secret. Global pull secret is used here. See also documentation about https://docs.openshift.com/container-platform/4.6/openshift_images/managing_images/using-image-pull-secrets.html#images-update-global-pull-secret_using-image-pull-secrets[using image pull secrets].

Update global pull secret:

* Open shell and use `oc login` to login to OpenShift using cluster administrator rights.
* Script link:external-registry/update_global_pull_secret.sh[update_global_pull_secret.sh] is used to add or edit global pull secret:
** `sh update_global_pull_secret.sh https://external-registry.forum.fi.ibm.com:5001 admin passw0rd`
* Global pull secret is rolled out to each node in the cluster.

==== Images

Push images to external registry:

* Pull image from public registry.
** If using airgapped OpenShift pull image from Internet, save it, copy to bastion and load it locally.
* Login to external registry, for example:
** `podman login -u admin -p passw0rd external-registry.forum.fi.ibm.com:5001`
* Tag image:
** `podman tag <image> external-registry.forum.fi.ibm.com:5001/<myimage>`
* Push image:
** `podman push external-registry.forum.fi.ibm.com:5001/<myimage>`
* Use image in YAML files etc.

== LDAP

LDAP used in this context is https://github.com/samisalkosuo/openldap-docker[OpenLDAP demo container] and it is running on bastion server.

https://docs.openshift.com/container-platform/4.6/authentication/identity_providers/configuring-ldap-identity-provider.html[OpenShift documentation about configuring identity providers].

Configure OpenShift to use LDAP identity provider:

* Have LDAP connection information.
** For example, https://github.com/samisalkosuo/openldap-docker#ldap-connection-and-filters[see OpenLDAP demo connection info].
* Edit link:ldap/add_ldap_identity_provider.sh[add_ldap_identity_provider.sh] to match your environment.
* Execute it:
** `sh add_ldap_identity_provider.sh`
** The commands adds new identity provider.
* Test configuration:
** Login as LDAP user: `oc login -u <user>`
** `oc whoami`

=== LDAP sync

Existing LDAPs typically include users and groups and it would be good to have those groups and users in OpenShift too.

https://access.redhat.com/documentation/en-us/openshift_container_platform/4.6/html/authentication_and_authorization/ldap-syncing[Syncing LDAP groups] does that.

* Edit link:ldap/ldapsync-config.yaml[ldapsync-config.yaml] to match your LDAP.
* Open terminal and login as cluster administrator.
* To see what is being done, and to check any errors, execute:
** `oc adm  groups sync --sync-config=ldapsync-config.yaml`
** The command prints out what it is going to do without doing it. 
** Output shows also any errors.
* Confirm changes and execute:
** `oc adm  groups sync --sync-config=ldapsync-config.yaml --confirm`
** This command syncs LDAP with OpenShift.
* Check groups and users:
** `oc get groups`
* Users in groups can login to OpenShift.

Executing sync is one-time task so it might be good to have a https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/[CronJob] inside OpenShift or a cron job outside OpenShift to periodically sync groups.

=== Cluster admins

By default, there are no cluster admin users when adding new identity provider or syncing LDAP groups.

Use existing cluster admin (kubeadmin for example) to add new cluster admins.

* Add individual user as cluster admin:
** `oc adm policy add-cluster-role-to-user cluster-admin <user>`
* Add a group as cluster-admins:
** `oc adm policy add-cluster-role-to-group cluster-admin <group>`

== HTPasswd identity provider

Steps to create HTPasswd identity provider is described here: https://docs.openshift.com/container-platform/4.6/authentication/identity_providers/configuring-htpasswd-identity-provider.html.

* Script link:htpasswd/htpasswd-util.sh[htpasswd-util.sh] is used to create/list/add/remove users in HTPasswd identity provider.
* When creating HTPasswd identity provider using the script, it creates 'cladmin'-user with random password and sets the user as cluster admin.
* Execute script:
** `sh htpasswd-util.sh`

== Certificates

After installing OpenShift, router uses self-signed certificate. Typical use case is to have a certificate signed by some Certificate Authority.

=== CA certificate

During installation, a custom CA certificate was created and it was added to _install-config.yaml_ and then it was added as user CA to OpenShift.

* Check custom CA:
** `oc -n openshift-config describe cm user-ca-bundle`
* However, custom CA is not trusted.
** Add custom CA as trusted CA:
** `oc patch proxy/cluster --type=merge --patch='{"spec":{"trustedCA":{"name":"user-ca-bundle"}}}'`
* If you need to add new CA certificate, use command:
** `oc -n openshift-config create configmap custom-ca --from-file=ca-bundle.crt=<ca cert file>``
     
=== Ingress certificate

Change ingress certificate:

* Prereq:
** Certificate for wildcard domain _*.apps.ocp-07.forum.fi.ibm.com_ exists and you have both _.crt_ and _.key_ files.
** Certificate is signed by CA, for example custom CA created during installation.
** Example files: _ocp_ingress.crt_ and _ocp_ingress.key_.
* Login as cluster admin.
* Add certificate as a secret:
** `oc -n openshift-ingress create secret tls custom-ingress-cert --cert=ocp_ingress.crt --key=ocp_ingress.key`
* Patch Ingress operator to use custom certificate:
** `oc patch --type=merge -n openshift-ingress-operator ingresscontrollers/default --patch '{"spec":{"defaultCertificate":{"name":"custom-ingress-cert"}}}'`
* Router pods are restarted and will reflect new Ingress certificate.

== Upgrade airgapped OpenShift

Upgrading airgapped OpenShift requires mirroring of updated images from Internet and then moving them to mirror registry in airgapped environment. https://docs.openshift.com/container-platform/4.6/updating/updating-restricted-network-cluster.html[Upgrade in airgapped environment is documented].

This assumes that OpenShift was installed using instructions in this repository so there is a mirror registry in the bastion server.

* Determine new version:
** Check OpenShift versions: https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/.
** Go to directory of desired OpenShift version.
** View _release.txt_ file and verify that desired version can upgrade existing version.
*** For example: https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/4.6.7/release.txt[v4.6.7 can upgrade v4.6.1].
* Login to jump server.
* Go to _install_-directory.
* Edit link:../install/upi-environment.sh[upi-environment.sh] and set _OCP_VERSION_ to the desired version.
** Source new configuration: `source upi-environment.sh`
* Download OpenShift images:
** `sh omg-upi-airgapped.sh download-ocp-update`
** If result is not _Success_, download images again.
* Package update images as tar-file:
** `sh omg-upi-airgapped.sh create-update-package`
** File `dist-$OCP_VERSION.tar` is created.
* Copy/move file to bastion.
* Login as cluster admin to OpenShift.
* Upload images to mirror registry and apply image signature file:
** `sh omg-upi-airgapped.sh upload-ocp-update <TARFILE>`
* After command completes, upgrade commans is printed. For example:
** `oc adm upgrade --allow-explicit-upgrade --to-image mirror-registry.forum.fi.ibm.com:5000/ocp/openshift4@sha256:4046047beed84bbba2c1762f130c402f9d05d584cb1dc3e8440f5809b2bb587e`
* Optionally verify that images are in the registry:
** `curl -u admin:passw0rd https://mirror-registry.forum.fi.ibm.com:5000/v2/ocp/openshift4/tags/list | jq .`
* Execute command to start OpenShift upgrade.
* Upgrade takes a moment...
* Monitor upgrade process by any of the following:
** OpenShift web console:
*** _Administration -> Cluster Settings_.
** `oc adm upgrade`
** `oc get clusteroperators`
** `oc get nodes`

== Disable the default OperatorHub sources

* Disable OperatorHub sources in airgapped installation:
** `oc patch OperatorHub cluster --type json -p '[{"op": "add", "path": "/spec/disableAllDefaultSources", "value": true}]'`

== Add worker node

Adding new worker node is straight-forward process.

* Have new server ready.
** Create new VM in virtualization environment
** Setup physical server.
** And so on.
* Get the MAC address of the server/network card.
** For example: `00:50:56:b3:7e:23`.
* Open link:../install/environment.sh[environment.sh]:
** Find environment variable `OCP_NODE_WORKER_HOSTS`.
** Add new server to the variable, for example:
** `worker-03 192.168.47.113 00:50:56:b3:7e:23;`
** Source new configuration: `source environment.sh`
* Configure DNS and DHCP/PXE:
** `sh omg.sh setup-dns`
** `sh omg.sh setup-dhcp`
* Power on VM or server.
** Server installs RHCOS from PXE and adds itself to OpenShift cluster.
* Two certificates needs to approved before worker node is ready.
** See certificate requests:
** `oc get csr`
** If any request in in 'Pending'-state, approve them:
** `oc adm certificate approve <csr name>`
** Approve all pending certificate requests using command:
*** `oc get csr |grep Pending |awk '{print "oc adm certificate approve " $1}' |sh`
* View node status using command:
** `oc get nodes`
* When new worker node shows _Ready_, then it is ready.

== Remove worker node

Remove worker node from cluster:

* Mark the node as unschedulable:
** `oc adm cordon <node_name>`
* Drain all Pods on your node:
** `oc adm drain <node_name> --force=true`
** or if it fails, use:
** `oc adm drain <node_name> --force --ignore-daemonsets --delete-local-data`
* Delete your node from the cluster:
** `oc delete node <node_name>`
* Shutdown node.
* Remove node IP and MAC address from configuration.
* Delete or otherwise dispose the node.

== Place router pods

It is possible to place pods in specific nodes using node selectors. Here we place router pods in two specific worker nodes. https://docs.openshift.com/container-platform/4.6/nodes/scheduling/nodes-scheduler-node-selectors.html[Procedure is documented].

* Open shell and use _oc_-command to login cluster admin.
* Choose two worker nodes to be dedicated for router pods.
* Add label to those worker nodes:
** `oc label node <node-name> nodeType=router`
* Router pods are in _openshift-ingress_project.
* Patch namespace and add annotation for node selector:
** `oc patch namespace openshift-ingress -p '{"metadata":{"annotations":{"openshift.io/node-selector":"nodeType=router"}}}'`
* Delete router pods to reschedule:
** `oc -n openshift-ingress get pods --no-headers |awk '{print "oc -n openshift-ingress delete pod " $1}' | sh`

== Label nodes

* Label nodes as infra-node:
** `oc label node <node> node-role.kubernetes.io/infra=""`
* Remove label, for example worker-label:
** `oc label node <node> node-role.kubernetes.io/worker-`

For example, label and taint dedicated storage (OpenShift Data Foundation) nodes:
```
oc label node storage01 node-role.kubernetes.io/infra=""
oc label node storage01 cluster.ocs.openshift.io/openshift-storage=""
oc adm taint node storage01 node.ocs.openshift.io/storage="true":NoSchedule
```

== Set/unset master nodes as workers

When selecting three-node cluster, master-nodes are also worker-nodes. If adding later new workers, it might be desirable to remove worker role from master-nodes.

* Remove worker role from masters:
** `oc patch schedulers.config.openshift.io/cluster --type merge -p '{"spec":{"mastersSchedulable":false}}'`
* And vice versa, add worker role to masters:
** `oc patch schedulers.config.openshift.io/cluster --type merge -p '{"spec":{"mastersSchedulable":true}}'`


== Backup etcd

See backup instructions in docs https://docs.openshift.com/container-platform/4.6/backup_and_restore/backing-up-etcd.html.

* Script link:backup/backup_etcd.sh[backup_etcd.sh] backs up etcd as described in documentation.
* Backup files are copied to local directory.
* Move files to location that is safe if disaster occurs.

== Restore etcd

Restore is documented: https://docs.openshift.com/container-platform/4.6/backup_and_restore/disaster_recovery/scenario-2-restoring-cluster-state.html.

The steps 1-8 in the documentation can be done using _restore_-scripts in the _backup_-directory. 

If using scripts, execute scripts in order and follow instructions

* Copy _snapshot*_ and _static_*_ backup files to directory where _restore_-scripts are.
* `sh restore_etcd_step_1.sh`
* `sh restore_etcd_step_2.sh`
* `sh restore_etcd_step_3.sh`
* `sh restore_etcd_step_4.sh`
* After executing step 4, https://docs.openshift.com/container-platform/4.6/backup_and_restore/disaster_recovery/scenario-2-restoring-cluster-state.html[go to documentation] and continue from step 9.

== Delete terminating namespace

Sometimes, when deleting namespace, it gets stuck at "Terminating".

* Find the root cause:
** Use https://github.com/thyarles/knsk
** Or check https://github.com/kubernetes/kubernetes/issues/60807#issuecomment-524772920
** Or https://www.openshift.com/blog/the-hidden-dangers-of-terminating-namespaces

But if namespace just needs to be removed, you can forcefully delete it.

* Edit namespace and remove all finalizers
** `oc edit namespace annoying-namespace`
* If it does not work, then continue.
* Get annoying namespace as JSON:
** `oc get namespace annoying-namespace -o json > tmp.json`
* Edit tmp.json
** Find _finalizers_ and remove all entries (usually "kubernetes")
* Apply tmp.json:
** `oc replace --raw "/api/v1/namespaces/annoying-namespace/finalize" -f ./tmp.json`
* Verify that namespace is deleted
** `oc get namespace |grep Terminating`