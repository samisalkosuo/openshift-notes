= OpenShift configuration
:toc: left
:toc-title: Table of Contents

This document describes how to do various operational tasks in OpenShift.

== Upgrade airgapped OpenShift

Upgrading airgapped OpenShift requires mirroring of updated images from Internet and then moving them to mirror registry in airgapped environment. https://docs.openshift.com/container-platform/4.6/updating/updating-restricted-network-cluster.html[Upgrade in airgapped environment is documented].

This assumes that OpenShift was installed using instructions in this repository so there is a mirror registry in the bastion server.

* Determine new version:
** Check OpenShift versions: https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/.
** Go to directory of desired OpenShift version.
** View _release.txt_ file and verify that desired version can upgrade existing version.
*** For example: https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/4.6.7/release.txt[v4.6.7 can upgrade v4.6.1].
* Login to jump server.
* Go to _install_-directory.
* Edit link:../install/environment.sh[environment.sh] and set _OCP_VERSION_ to the desired version.
** Source new configuration: `source environment.sh`
* Create package that includes images of the new OpenShift version:
** `sh omg.sh create-update-package`
** Image tar file is created with name `images_<OCP_VERSION>_.tar`
* Copy/move tar-file to bastion-server.
* Login to bastion-server as root.
* Login to OpenShift-cluster as cluster admin.
* Go to _dist_-directory (created during installation).
* Edit link:../install/environment.sh[environment.sh] and set _OCP_VERSION_ to the desired version.
** Source new configuration: `source environment.sh`
* Extract image tar file and upload images to mirror registry.
** `sh omg.sh upload-update-images`
** Command extract mirror image tar file, pushes images to image registry and applies the mirrored release image signature config map to OpenShift cluster.
* After command completes, an upgrade command is printed. For example:
** `oc adm upgrade --allow-explicit-upgrade --to-image mirror-registry.forum.fi.ibm.com:5000/ocp/openshift4@sha256:6ddbf56b7f9776c0498f23a54b65a06b3b846c1012200c5609c4bb716b6bdcdf`
* Optionally verify that images are in the registry:
** `curl -u admin:passw0rd https://mirror-registry.forum.fi.ibm.com:5000/v2/ocp/openshift4/tags/list | jq`
* Execute command to start OpenShift upgrade.
* Upgrade takes a moment...
* Monitor upgrade process by any of the following:
** OpenShift web console:
*** _Administration -> Cluster Settings_.
** `oc adm upgrade`
** `oc get clusteroperators`
** `oc get nodes`


== Add worker node

Adding new worker node is straight-forward process.

* Have new server ready.
** Create new VM in virtualization environment
** Setup physical server.
** And so on.
* Get the MAC address of the server/network card.
** For example: `00:50:56:b3:7e:23`.
* Open link:../install/environment.sh[environment.sh]:
** Find environment variable `OCP_NODE_WORKER_HOSTS`.
** Add new server to the variable, for example:
** `worker-03 192.168.47.113 00:50:56:b3:7e:23;`
** Source new configuration: `source environment.sh`
* Configure DNS and DHCP/PXE:
** `sh omg.sh setup-dns`
** `sh omg.sh setup-dhcp`
* Power on VM or server.
** Server installs RHCOS from PXE and adds itself to OpenShift cluster.
* Two certificates needs to approved before worker node is ready.
** See certificate requests:
** `oc get csr`
** If any request in in 'Pending'-state, approve them:
** `oc adm certificate approve <csr name>`
* View node status using command:
** `oc get nodes`
* When new worker node shows _Ready_, then it is ready.

== Remove worker node

Remove worker node from cluster:

* Mark the node as unschedulable:
** `oc adm cordon <node_name>`
* Drain all Pods on your node:
** `oc adm drain <node_name> --force=true`
** or if it fails, use:
** `oc adm drain <node_name> --force --ignore-daemonsets --delete-local-data`
* Delete your node from the cluster:
** `oc delete node <node_name>`
* Shutdown node.
* Remove node IP and MAC address from configuration.
* Delete or otherwise dispose the node.

== Place router pods

It is possible to place pods in specific nodes using node selectors. Here we place router pods in two specific worker nodes. https://docs.openshift.com/container-platform/4.6/nodes/scheduling/nodes-scheduler-node-selectors.html[Procedure is documented].

* Open shell and use _oc_-command to login cluster admin.
* Choose two worker nodes to be dedicated for router pods.
* Add label to those worker nodes:
** `oc label node <node-name> nodeType=router`
* Router pods are in _openshift-ingress_project.
* Patch namespace and add annotation for node selector:
** `oc patch namespace openshift-ingress -p '{"metadata":{"annotations":{"openshift.io/node-selector":"nodeType=router"}}}'`
* Delete router pods to reschedule:
** `oc -n openshift-ingress get pods --no-headers |awk '{print "oc -n openshift-ingress delete pod " $1}' | sh`

== Set/unset master nodes as workers

When selecting three-node cluster, master-nodes are also worker-nodes. If adding later new workers, it might be desirable to remove worker role from master-nodes.

* Remove worker role from masters:
** `oc patch schedulers.config.openshift.io/cluster --type merge -p '{"spec":{"mastersSchedulable":false}}'`
* And vice versa, add worker role to masters:
** `oc patch schedulers.config.openshift.io/cluster --type merge -p '{"spec":{"mastersSchedulable":true}}'`


== Backup etcd

See backup instructions in docs https://docs.openshift.com/container-platform/4.6/backup_and_restore/backing-up-etcd.html.

* Script link:backup/backup_etcd.sh[backup_etcd.sh] backs up etcd as described in documentation.
* Backup files are copied to local directory.
* Move files to location that is safe if disaster occurs.

== Restore etcd

Restore is documented: https://docs.openshift.com/container-platform/4.6/backup_and_restore/disaster_recovery/scenario-2-restoring-cluster-state.html.

The steps 1-8 in the documentation can be done using _restore_-scripts in the _backup_-directory. 

If using scripts, execute scripts in order and follow instructions

* `sh restore_etcd_step_1.sh`
* `sh restore_etcd_step_2.sh`
* `sh restore_etcd_step_3.sh`
* `sh restore_etcd_step_4.sh`
* After executing step 4, https://docs.openshift.com/container-platform/4.6/backup_and_restore/disaster_recovery/scenario-2-restoring-cluster-state.html[go to documentation] and continue from step 9.

