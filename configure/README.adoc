= OpenShift configuration
:toc: left
:toc-title: Table of Contents

This document describes how to do various configurations in OpenShift.

== NFS client provisioner

https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner[Kubernetes NFS-Client Provisioner] provides dynamic provisioning using existing NFS server. Great way to provide initial storage class for OpenShift.

=== Online installation

* Follow instructions in https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner.
* After storage class is created, optionally set _managed-nfs-storage_-storageclass as default:
** `oc patch storageclass managed-nfs-storage -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'`

=== Airgapped installation

Since OpenShift does not have access to Internet, extra steps are required to install provisioner.

* Login to jump server.
* Package provisioner image and deplyment files using link:nfs-client-provisioner/package_provisioner_image.sh[save_provisioner_image.sh]:
** `sh package_provisioner_image.sh`
* Copy _nfs-client-provisioner-files.tar_ to bastion server.
* Login to bastion server.
* Extract _nfs-client-provisioner-files.tar_:
** `tar -xf nfs-client-provisioner-files.tar`
* Change to _nfs-client-provisioner_-directory.
* Load provisioner image:
** `podman load < nfs-client-provisioner.tar`
* Tag image:
** `podman tag nfs-client-provisioner external-registry.forum.fi.ibm.com:5001/nfs-client-provisioner:latest`
* Push image:
** `podman push external-registry.forum.fi.ibm.com:5001/nfs-client-provisioner:latest`
* If you have NFS server, you need info about it.
** Or set your own NFS as described in the next section.
* Open _deployment.yaml_.
* Find _image_-entry and change to correct provisioner image. For example:
** `external-registry.forum.fi.ibm.com:5001/nfs-client-provisioner:latest`
* Set IP address and path of your NFS server.
* Login as cluster administrator using oc-client.
* Setup provisioner:
** `oc create -f rbac.yaml`
** `oc create -f deployment.yaml`
** `oc create -f class.yaml`
* Test by creating persistent volume claim:
** `oc create -f test-claim.yaml`
** There should test-claim directory in the NFS server.
* Get storage class:
** `oc get sc`
** And you should see _managed-nfs-storage_-storageclass.
* Optionally set _managed-nfs-storage_-storageclass as default:
** `oc patch storageclass managed-nfs-storage -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'`

==== NFS server

As an example, NFS server can be installed on the bastion server.

* Execute script link:nfs-client-provisioner/setup_nfs_server.sh[setup_nfs_server.sh]:
** `sh setup_nfs_server.sh`
** Script sets up NFS server using directory _/mnt/nfs_share_.
** NFS server allows clients from OpenShift internal network. Internal network is set using _OCP_DHCP_NETWORK_- environment variable set in link:../config.sh[config.sh].

== Image registry

=== Internal registry

Internal image registry is not initially available in OpenShift UPI online or airgapped installations.

* Image registry requires persistent storage before it can be made available.
* Configure storageclass for dynamic provisioning.
** For example, NFS provisioner described previously.
* Configure a default storageclass (like the NFS provisioner).
* Patch image registry operator configuration:
** `oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch '{"spec":{"managementState":"Managed","defaultRoute":true,"storage":{"pvc":{"claim":""}}}}'`
* The patch-command creates also default route for the registry.
** Get default route using command:
** `oc get route default-route -n openshift-image-registry --template='{{ .spec.host }}'`
* Registry can not be used until an identity provider (for example LDAP or HTPasswd) has been configured.
** See later sections about LDAP and HTPasswd identity provider.

=== External registry

External registry is an image registry for containers that should be available for OpenShift but, for any reason, not available from public registry or internal image registry.

The most obvious use case for external registry is for the airgapped OpenShift installation.
The registry is created using script link:external-registry/create-registry.sh[create-registry.sh].

* Create external registry using command:
** `sh create-registry.sh <REGISTRY_NAME> <REGISTRY_DIR> <REGISTRY_PORT> <REGISTRY_CRT_FILE_PATH> <REGISTRY_KEY_FILE_PATH>`
** _REGISTRY_NAME_ is the name of the systemd service.
** _REGISTRY_DIR_ is the full path to registry dir. It is created if it does not exist.
** _REGISTRY_PORT_ is registry port.
** _REGISTRY_CRT_FILE_PATH_ is the full path to certificate file.
** _REGISTRY_KEY_FILE_PATH_ is the full path to certificate key file.
* Registry container is controlled using systemctl.

==== Configure OpenShift

When using external registry in OpenShift, pull secret is required so that pods can pull images from the registry.

Pull secret can be added for a project or it can be added as global cluster pull secret. Global pull secret is used here. See also documentation about https://docs.openshift.com/container-platform/4.6/openshift_images/managing_images/using-image-pull-secrets.html#images-update-global-pull-secret_using-image-pull-secrets[using image pull secrets].

Update global pull secret:

* Open shell and use `oc login` to login to OpenShift using cluster administrator rights.
* Script link:external-registry/update_global_pull_secret.sh[update_global_pull_secret.sh] is used to add or edit global pull secret:
** `sh update_global_pull_secret.sh https://external-registry.forum.fi.ibm.com:5001 admin passw0rd`
* Global pull secret is rolled out to each node in the cluster.

==== Images

Push images to external registry:

* Pull image from public registry.
** If using airgapped OpenShift pull image from Internet, save it, copy to bastion and load it locally.
* Login to external registry, for example:
** `podman login -u admin -p passw0rd external-registry.forum.fi.ibm.com:5001`
* Tag image:
** `podman tag <image> external-registry.forum.fi.ibm.com:5001/<myimage>`
* Push image:
** `podman push external-registry.forum.fi.ibm.com:5001/<myimage>`
* Use image in YAML files etc.

== LDAP

LDAP used in this context is https://github.com/samisalkosuo/openldap-docker[OpenLDAP demo container] and it is running on bastion server.

https://docs.openshift.com/container-platform/4.6/authentication/identity_providers/configuring-ldap-identity-provider.html[OpenShift documentation about configuring identity providers].

Configure OpenShift to use LDAP identity provider:

* Have LDAP connection information.
** For example, https://github.com/samisalkosuo/openldap-docker#ldap-connection-and-filters[see OpenLDAP demo connection info].
* Edit link:ldap/configure_ldap_idp.sh[configure_ldap_idp.sh] to match your environment.
* Execute it:
** `sh configure_ldap_idp.sh`
* Test configuration:
** Login as LDAP user: `oc login -u <user>`
** `oc whoami`

=== Cluster admin user

By default, there are no cluster admin users when adding new identity provider.

* As cluster admin, such as _kubeadmin_, add new cluster admin user:
** `oc adm policy add-cluster-role-to-user cluster-admin <user>`

== HTPasswd identity provider

Steps to create HTPasswd identity provider is described here: https://docs.openshift.com/container-platform/4.6/authentication/identity_providers/configuring-htpasswd-identity-provider.html.

* Script link:htpasswd/htpasswd-util.sh[htpasswd-util.sh] is used to create/list/add/remove users in HTPasswd identity provider.
* When creating HTPasswd identity provider using the script, it creates 'cladmin'-user with random password and sets the user as cluster admin.
* Execute script:
** `sh htpasswd-util.sh`

== Certificates

After installing OpenShift, router uses self-signed certificate. Typical use case is to have a certificate signed by some Certificate Authority.

=== CA certificate

During installation, a custom CA certificate was created and it was added to _install-config.yaml_ and then it was added as user CA to OpenShift.

* Check custom CA:
** `oc -n openshift-config describe cm user-ca-bundle`
* However, custom CA is not trusted.
** Add custom CA as trusted CA:
** `oc patch proxy/cluster --type=merge --patch='{"spec":{"trustedCA":{"name":"user-ca-bundle"}}}'`
* If you need to add new CA certificate, use command:
** `oc -n openshift-config create configmap custom-ca --from-file=ca-bundle.crt=<ca cert file>``
     
=== Ingress certificate

Change ingress certificate:

* Prereq:
** Certificate for wildcard domain _*.apps.ocp-07.forum.fi.ibm.com_ exists and you have both _.crt_ and _.key_ files.
** Certificate is signed by CA, for example custom CA created during installation.
** Example files: _ocp_ingress.crt_ and _ocp_ingress.key_.
* Login as cluster admin.
* Add certificate as a secret:
** `oc -n openshift-ingress create secret tls custom-ingress-cert --cert=ocp_ingress.crt --key=ocp_ingress.key`
* Patch Ingress operator to use custom certificate:
** `oc patch --type=merge -n openshift-ingress-operator ingresscontrollers/default --patch '{"spec":{"defaultCertificate":{"name":"custom-ingress-cert"}}}'`
* Router pods are restarted and will reflect new Ingress certificate.